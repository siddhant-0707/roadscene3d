# RoadScene3D: A Self-Supervised 3D Scene Understanding & Flywheel Pipeline

A lightweight end-to-end 3D perception system that learns from RGB + LiDAR data, produces 3D bounding boxes, uses self-supervised pretraining, and includes an automated retraining flywheel with CI/CD integration.

## ğŸ¯ Project Overview

This project implements a complete 3D object detection pipeline optimized for autonomous driving applications, with focus on:

- **3D Object Detection**: PointPillars-based detection on nuScenes dataset
- **Self-Supervised Learning**: Contrastive pretraining on LiDAR data
- **Model Optimization**: Quantization and OpenVINO export for deployment
- **Automated Flywheel**: Active learning loop with CI/CD integration
- **Production-Ready**: MLflow tracking, telemetry, and monitoring

## ğŸ—ï¸ Architecture

```
Raw Sensor Data (RGB + LiDAR)
    â†“
Preprocessing & Fusion
    â†“
Self-Supervised Pretraining (Contrastive)
    â†“
3D Detection Network (PointPillars)
    â†“
Evaluation Module (mAP@IoU, Latency)
    â†“
Model Registry + Metadata Store
    â†“
Automated Retrain Trigger (New Data/Active Learning)
```

## ğŸ“‹ Requirements

- **Hardware**:
  - GPU: NVIDIA RTX 4070 (8GB VRAM) or similar
  - RAM: 32GB recommended
  - Storage: ~5GB for nuScenes mini dataset (or ~400GB for full dataset)

- **Software**:
  - Python 3.9+
  - CUDA 11.8+ (for GPU training)
  - nuScenes dataset access (sign up at https://www.nuscenes.org/signup)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone <repository-url>
cd roadscene3d

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install MMDetection3D (follow official installation guide)
# See: https://mmdetection3d.readthedocs.io/en/latest/get_started.html
```

### 2. Dataset Setup

```bash
# Download nuScenes mini dataset (~4GB)
# See detailed instructions in NUSCENES_DOWNLOAD.md
python scripts/download_nuscenes.py --output-dir data/nuscenes

# Or manually download from https://www.nuscenes.org/download
# Extract to data/nuscenes/v1.0-mini/
```

### 3. Training

```bash
# Train baseline model with memory-optimized settings
python src/training/train.py \
    --config configs/pointpillars_nuscenes_8gb.py \
    --work-dir work_dirs/pointpillars \
    --gpu-id 0
```

### 4. Evaluation

```python
from src.evaluation.metrics import evaluate_model
import torch

# Load model and dataloader
# ...

# Evaluate
metrics = evaluate_model(model, dataloader, device)
print(f"mAP@0.7: {metrics['mAP@0.7']:.4f}")
```

## ğŸ“ Project Structure

```
roadscene3d/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ nuscenes/       # nuScenes dataset (downloaded)
â”‚   â””â”€â”€ processed/      # Preprocessed data
â”œâ”€â”€ configs/            # Model & training configs
â”‚   â”œâ”€â”€ pointpillars_nuscenes_8gb.py  # Primary config for nuScenes
â”‚   â””â”€â”€ pointpillars_8gb.py            # Legacy Waymo config
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/           # Data loaders & preprocessing
â”‚   â”‚   â”œâ”€â”€ nuscenes_loader.py  # nuScenes dataset loader (primary)
â”‚   â”‚   â””â”€â”€ waymo_loader.py     # Waymo loader (optional, for future use)
â”‚   â”œâ”€â”€ training/       # Training scripts
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â”œâ”€â”€ evaluation/     # Metrics & evaluation
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ pretraining/    # Self-supervised pretraining
â”‚   â”œâ”€â”€ optimization/   # Quantization & OpenVINO export
â”‚   â”œâ”€â”€ flywheel/       # Active learning & retraining
â”‚   â””â”€â”€ utils/          # Utilities
â”‚       â””â”€â”€ memory_monitor.py
â”œâ”€â”€ scripts/            # Data download & preprocessing scripts
â”‚   â””â”€â”€ select_waymo_subset.py
â”œâ”€â”€ ci/                 # GitHub Actions workflows
â”œâ”€â”€ dashboard/          # Streamlit/Gradio dashboard
â”œâ”€â”€ tests/              # Unit & integration tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ”§ Hardware Optimization

This project is optimized for **RTX 4070 (8GB VRAM)**:

- **Batch Size**: 2 (with gradient accumulation)
- **Mixed Precision**: FP16 training (required)
- **Gradient Accumulation**: 4 steps (effective batch size = 8)
- **Expected VRAM**: ~6-7GB peak during training
- **Expected RAM**: ~8-12GB during training

Adjust `configs/pointpillars_nuscenes_8gb.py` if you have different hardware.

## ğŸ“Š Phase Implementation

### Phase 1: Dataset & Baseline Model âœ…
- [x] Environment setup
- [x] Waymo dataset integration
- [x] Baseline 3D detection model (PointPillars)
- [x] Evaluation framework

### Phase 2: Optimization & Export
- [ ] Model quantization (INT8)
- [ ] OpenVINO export
- [ ] Telemetry & logging

### Phase 3: Automated Flywheel
- [ ] Active learning pipeline
- [ ] Retraining automation
- [ ] CI/CD integration

### Phase 4: Visualization & Documentation
- [ ] Streamlit dashboard
- [ ] Self-supervised pretraining
- [ ] Documentation & article

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src
```

## ğŸ“ˆ Monitoring

The project uses MLflow for experiment tracking:

```bash
# Start MLflow UI
mlflow ui --backend-store-uri ./mlruns

# View at http://localhost:5000
```

## ğŸ¤ Contributing

This is a portfolio project. For questions or improvements, please open an issue.

## ğŸ“ License

[Specify your license]

## ğŸ™ Acknowledgments

- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) for the 3D detection framework
- [nuScenes Dataset](https://www.nuscenes.org/) for the dataset
- Open source community for tools and libraries

## ğŸ“š References

- [PointPillars Paper](https://arxiv.org/abs/1812.05784)
- [nuScenes Dataset](https://www.nuscenes.org/)
- [nuScenes DevKit](https://github.com/nutonomy/nuscenes-devkit)
- [MMDetection3D Documentation](https://mmdetection3d.readthedocs.io/)
